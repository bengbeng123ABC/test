import pandas as pd
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
# from sklearn.xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from matplotlib import pyplot

#读取文件
data = pd.read_csv('gcc.csv')
# data1 = data['emd_lable2']
X3 = data


X3.groupby(X3['emd_lable2']).size()
emd_lable2
0.0    21957
1.0    21957
dtype: int64

X3.drop(['pax_name'],axis=1,inplace=True)

# 训练集矩阵
# array = X3.values

Y = X3.pop('emd_lable2')
X = X3.values

X
array([[0.00000000e+00, 8.09123633e-04, 5.95642896e-04, ...,
        1.23579143e-03, 0.00000000e+00, 0.00000000e+00],
       [1.00000000e+00, 3.44455213e-04, 3.91623479e-04, ...,
        1.23579143e-03, 0.00000000e+00, 0.00000000e+00],
       [2.00000000e+00, 4.17153537e-04, 5.69847337e-05, ...,
        1.23579143e-03, 0.00000000e+00, 0.00000000e+00],
       ...,
       [4.39110000e+04, 3.64709954e-04, 3.98658631e-05, ...,
        1.23579143e-03, 0.00000000e+00, 0.00000000e+00],
       [4.39120000e+04, 6.29281462e-04, 1.78223859e-04, ...,
        8.58004898e-03, 7.50180618e-02, 6.99300699e-03],
       [4.39130000e+04, 3.17333518e-04, 3.15409329e-04, ...,
        1.23579143e-03, 0.00000000e+00, 0.00000000e+00]])
Y
0        0.0
1        0.0
2        0.0
3        0.0
4        1.0
        ... 
43909    1.0
43910    1.0
43911    1.0
43912    1.0
43913    1.0
Name: emd_lable2, Length: 43914, dtype: float64

folds = 10
seed = 7
kfold = KFold(n_splits=folds,random_state=seed)

for name in models:
    result = cross_val_score(models[name], X , Y, cv=kfold)
    results.append(result)
    msg = '%s:%.3f(%.3f)'%(name,result.mean(),result.std())
    print(msg)
models = {}
models['LR'] = LogisticRegression()
models['LDA'] = LinearDiscriminantAnalysis()
models['KNN'] = KNeighborsClassifier()
models['CART'] = DecisionTreeClassifier()
models['SVM'] = SVC()
models['NB'] = GaussianNB()
models['XGboost'] = XGBClassifier()
results = []


LR:0.769(0.362)
LDA:0.930(0.078)
KNN:0.950(0.054)
CART:0.923(0.067)
SVM:0.940(0.079)
NB:0.887(0.164)
[16:15:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[16:15:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
XGboost:0.952(0.050)
fig = pyplot.figure()
fig.suptitle('算法模型比较')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(models.keys())
pyplot.show()
fig = pyplot.figure()
fig.suptitle('算法模型比较')

fig = pyplot.figure()
fig.suptitle('算法模型比较')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(models.keys())
pyplot.show()
